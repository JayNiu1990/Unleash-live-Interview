{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfa8309-a0d0-4714-9787-55523dc262b4",
   "metadata": {},
   "source": [
    "# **Technical Report documented by Yufu Niu**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a7a6e-01fc-451a-b595-87b28d4961d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Problem Overview**\n",
    "### **The object of this assignment is to locate the vehicles in the bev image. The provided image data is displayed as follow:**\n",
    "**1. The Fixed camera view image with marked lane lines and stop lines (red lines) plus measured physical distance**  \n",
    "**2. The cooresponding Google earth bev image with marked lane lines and stop lines (red lines) plus measured physical distance**  \n",
    "**3. kml file**  \n",
    "**4. The Fixed camera video**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a5992-ca11-497a-882d-7bb9c52192b7",
   "metadata": {},
   "source": [
    "## **Workflow**\n",
    "**1: Load object detection model**  \n",
    "**2: Create a Video Writer for saving frames to a new video file**  \n",
    "**3: Load background subtractor**  \n",
    "**4: Process video frame by frame**  \n",
    "**5: Load the BEV image**  \n",
    "**6: Manually record the paired pixel locations for marked red line from camera image and BEV image using ImageJ**  \n",
    "**7: Calculate Homography**  \n",
    "**8: Map the bounding box locations from camera coordinate to bev coordinate**\n",
    "**9: draw vehicle locations on bev image**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe58dc-eb8d-4bc6-9381-bd4eb9d5f937",
   "metadata": {},
   "source": [
    "## **Python implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e22444-7add-45ec-8c83-d882348bc9c9",
   "metadata": {},
   "source": [
    "## **Step1: Load object detection model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1a0136-f867-4b58-896b-3ef33bf99123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26a460-3208-4b93-b1d9-9e4184e79501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load yolo - try yolo models at different scales\n",
    "#model = YOLO('yolov8s.pt')\n",
    "#model = YOLO('yolov8m.pt')\n",
    "model = YOLO('yolov8x.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5ee8c-cc79-4715-83ae-08da48ddbbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./video_01.mp4')\n",
    "#check if video has been successfully loaded or not\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    raise ValueError(\"Failed to read video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d355e15-8cc4-49ab-8506-60293786bfd0",
   "metadata": {},
   "source": [
    "## **Step2: Create a video writer for saving frames to a new video file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecff4dc-a662-4696-8a63-185ca8dfbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check video information\n",
    "height, width, channels = frame.shape\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "output_video = cv2.VideoWriter('./results/yolo_annotated_video_01.mp4', fourcc, fps, (width, height))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7097fe7-9122-4dc2-a225-af465839422a",
   "metadata": {},
   "source": [
    "## **Step3: Load background subtractor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3712e50b-3d02-4bcf-98ea-cb9334b85406",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = cv2.createBackgroundSubtractorMOG2(\n",
    "    history = 50, varThreshold = 50, detectShadows = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058b61e-0f84-4bab-a248-1bc2ae7546bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vehicle class and color\n",
    "vehicle_classes = {\n",
    "    'car': {'color': (0, 255, 0), 'radius': 4},    # green\n",
    "    'truck': {'color': (0, 0, 255), 'radius': 6},  # red\n",
    "    'bus': {'color': (255, 0, 0), 'radius': 8}     # blue\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef67a7-bb3f-42bb-8b2c-d26445bd0829",
   "metadata": {},
   "source": [
    "## **Step4: Process video frame by frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66f394-f60b-42c7-a958-7c82ddabc805",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frames_boxes = []  # list to store info for each frame\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        \n",
    "    bgmask = bg.apply(frame) # apply background substractor to frame\n",
    "    bgmask = cv2.medianBlur(bgmask,5) #apply median method to remove small noisy dots\n",
    "    \n",
    "    # Run YOLOv8 prediction\n",
    "    results = model.predict(\n",
    "        source=frame,      # input frame\n",
    "        imgsz=640,         # resize for model\n",
    "        conf=0.25,         # confidence threshold\n",
    "        iou=0.45,          # IoU threshold\n",
    "        device='cpu',      # Using CPU\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    # Get the first result (single frame)\n",
    "    res = results[0]\n",
    "    frame_boxes = []\n",
    "    \n",
    "    # Draw boxes for detected vehicles\n",
    "    if hasattr(res, 'boxes') and len(res.boxes): #check if 'boxes' is in res and 'boxes' is not empty\n",
    "        for box in res.boxes:\n",
    "            # Bounding box coordinates\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist()) #convert bounding box coordiantes to list and remove decimals\n",
    "            confidence = float(box.conf[0]) #calculate confidence\n",
    "            class_id = int(box.cls[0]) #get the class id\n",
    "            class_name = model.names.get(class_id, str(class_id)) #get the related vehicle type name\n",
    "\n",
    "            # Only draw vehicle classes\n",
    "            if class_name in vehicle_classes:\n",
    "                motion_region = bgmask[y1:y2,x1:x2] #get the bounding box region\n",
    "                motion_ratio = np.mean(motion_region > 0) # convert to boolean for static background search\n",
    "                if motion_ratio < 0.03: #if motion_ratio <0.03, it is a static background, and do not process it further\n",
    "                    continue\n",
    "\n",
    "                color = vehicle_classes[class_name]['color'] #get color for each vehicle class\n",
    "                \n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2) #draw a rectangle to locate each behicle\n",
    "                cv2.putText(frame, f\"{class_name} {confidence:.2f}\", (x1, y1 - 6), #put text for each rectangle\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "                # Add to 2D plane\n",
    "                radius = vehicle_classes[class_name]['radius'] #calculate bottom-center coordinate\n",
    "                x_center = int((x1 + x2) / 2)\n",
    "                y_bottom = int(y2)\n",
    "\n",
    "                frame_boxes.append({\n",
    "                    'class': class_name,\n",
    "                    'conf': confidence,\n",
    "                    'bbox': [x1, y1, x2, y2],\n",
    "                    'center': [x_center, y_bottom]\n",
    "                })\n",
    "\n",
    "    all_frames_boxes.append(frame_boxes)\n",
    "    output_video.write(frame)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "output_video.release()\n",
    "# Save to JSON\n",
    "with open('./results/vehicle_bboxes.json', 'w') as f:\n",
    "    json.dump(all_frames_boxes, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8a4c8-5f4c-41b6-aeeb-bb082b5701de",
   "metadata": {},
   "source": [
    "## **Step5: Load the BEV image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94671543-ae80-4fe2-8d7a-767426d978d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BEV image\n",
    "bev_image = cv2.imread('2020-25-06.png')  # or .jpg\n",
    "plane_image = bev_image.copy()  # reset per frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e941ae6-193a-426c-b1a9-47494dac0bbd",
   "metadata": {},
   "source": [
    "## **Step6: Manually record the paired pixel locations for marked red line from camera image and BEV image using ImageJ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1d72e9f-ea7c-4747-a967-21415151d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_coordiate = np.array([[551,1032],[684,926],[872,766],[1133,503],[1105,905],[1155,835],\n",
    "                            [1264,682],[1295,636],[1148,504],[1625,598],[1161,914],[1848,1020],[703,354],[727,397]])\n",
    "bev_coordiante = np.array([[506,262],[527,328],[566,394],[691,605],[456,416],[473,447],\n",
    "                          [513,535],[532,572],[683,610],[449,715],[320,541],[244,544],[1168,544],[1044,520]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e57bd-66ab-4624-88d0-e4a9c87caf2e",
   "metadata": {},
   "source": [
    "## **Step7: Calculate Homography**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0e54565-1f68-4729-8f5f-d3abfb6665b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the transformation matrix from camera image to bev image\n",
    "H, status = cv2.findHomography(camera_coordiate, bev_coordiante)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ffc4a-cf1e-4084-ae08-fbaf633ee275",
   "metadata": {},
   "source": [
    "## **Step8: Map the bounding box locations from camera coordinate to bev coordinate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3c6b34f-522b-4b49-8b55-aaf16bc10e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change bounding box locations to bottom-center and map coordinates to BEV image\n",
    "bev_coords_all_frames = []\n",
    "\n",
    "for frame_boxes in all_frames_boxes:\n",
    "    frame_bev = []\n",
    "    for box in frame_boxes:\n",
    "        x1, y1, x2, y2 = box['bbox']\n",
    "        x_center = (x1 + x2) / 2\n",
    "        y_bottom = y2\n",
    "        vehicle_pixel = np.array([x_center, y_bottom, 1])\n",
    "        \n",
    "        # Map to BEV\n",
    "        vehicle_bev = np.dot(H, vehicle_pixel) #apply a homography H to a point:\n",
    "        vehicle_bev /= vehicle_bev[2]  # normalize\n",
    "\n",
    "        frame_bev.append({\n",
    "            'class': box['class'],\n",
    "            'conf': box['conf'],\n",
    "            'bev_x': vehicle_bev[0],\n",
    "            'bev_y': vehicle_bev[1]\n",
    "        })\n",
    "    bev_coords_all_frames.append(frame_bev)\n",
    "    \n",
    "# Save to JSON\n",
    "with open('./results/vehicle_bev_coords.json', 'w') as f:\n",
    "    json.dump(bev_coords_all_frames, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef911e5e-b7ef-4a50-bc7c-1da8b454b765",
   "metadata": {},
   "source": [
    "## **Step9: draw vehicle locations on bev image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d09c04d1-6b88-4acf-9d4b-68656b1f9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the single frame and draw vehicle locations on bev image\n",
    "# Show the 201th frame as example\n",
    "frame_idx = 201\n",
    "frame_bev = bev_coords_all_frames[frame_idx]  # list of dicts with 'bev_x', 'bev_y'\n",
    "for v in frame_bev:\n",
    "    x_px = int(v['bev_x'])\n",
    "    y_px = int(v['bev_y'])\n",
    "    cls = v['class']\n",
    "    color = vehicle_classes.get(cls, {'color': (0,0,255)})['color']\n",
    "    radius = vehicle_classes.get(cls, {'radius': 5})['radius']\n",
    "    cv2.circle(plane_image, (x_px, y_px), radius, color, -1)\n",
    "    cv2.putText(plane_image, cls, (x_px-20, y_px-20),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1)\n",
    "# Save the 2D plane visualization\n",
    "cv2.imwrite(f'./results/bev_with_vehicles_{frame_idx}.png', plane_image)\n",
    "\n",
    "# save the cooresponding frame for comparsion\n",
    "cap = cv2.VideoCapture('./video_01.mp4')\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "ret, camera_frame = cap.read()\n",
    "\n",
    "cv2.imwrite(f'./results/camera_frame_{frame_idx}.png', camera_frame)\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8546dd9-0959-4f34-af81-1a4b8b1fe329",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "**In this report, a yolov8 model was used to detect vehicle objects on a fixed carema video. Background subtractor was applied to detect static background for removing misleading objects such as trees, adveristing board. After that, ImageJ was used to manually record the paired pixel locations for marked red line from camera image and BEV image for calculating Homography. In this way, a coorindate mapping can be built from fixed camera image to google earth bev image. Finally, the transformed location of the detected vehicles were drawn on the bev image.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ffc00-4970-4a38-ac51-73662b65fd31",
   "metadata": {},
   "source": [
    "## **Further Work for Improvement Suggestions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1d906-9c61-4ac5-9892-7dc35666ffc9",
   "metadata": {},
   "source": [
    "## **The proposed workflow can be further improved by the following suggestions:** \n",
    "**1. The current homography was calculated by pixel locations which may be not accurate enough. Further work can be related to utilise kml files to extract geospatial information. In this way, the pixel location can be mapped to GPS location which will provide higher accuracy.**  \n",
    "**2. The physical distance was not utilised, the physical distance can verfy the accuracy of the manual label from ImageJ by comparing the physical distance between two points on camera image and bev image. In addition, the location of the vehicles on bev image can be scaled using the physical distance. This is very important when estimating real-time speed on the bev image.**  \n",
    "**3. More powerful deep learning model can be tried to further improve the accuracy of the object detection**  \n",
    "**4. Currently only dots are used to locate the vehicle in the bev image, bounding box or polygen can further constrain the shape of the vehicle on the bev image.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e27e6c-ce20-4fcb-b327-2e6070c295cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
